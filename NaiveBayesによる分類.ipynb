{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVG+la+Ly4Kji0CHqer2vV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TKph/colab/blob/main/NaiveBayes%E3%81%AB%E3%82%88%E3%82%8B%E5%88%86%E9%A1%9E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# データの読み込み\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "owHMMpyIQMRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kXhgexv8O-Lu"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from pprint import pprint\n",
        "\n",
        "train_set = fetch_20newsgroups(subset = 'train', random_state = 42)\n",
        "test_set = fetch_20newsgroups(subset = 'test', random_state = 42)\n",
        "\n",
        "X_train = train_set.data\n",
        "y_train = train_set.target\n",
        "X_test = test_set.data\n",
        "y_test = test_set.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 事前準備\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vulF6qQBS-pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('カテゴリ一覧')\n",
        "pprint(train_set.target_names)\n",
        "print('\\n')\n",
        "print(f'News Script: \\n {X_train[0]}')\n",
        "print('記事その１のカテゴリ')\n",
        "print(f'Text Category label: {y_train[0]}')"
      ],
      "metadata": {
        "id": "X3OuGDfBTBhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag-of-Word表現の実装\n",
        "\n",
        "BoW表現では\n",
        "'This is an apple. I eat this apple' は (This, is , an , apple, I eat) = (2, 1, 1, 2, 1, 1)と表現される. またカテゴリに関係なく出現する単語(this,that,it,a,an,I,be動詞など)をstop wordとして登録しておいてそれらを除くと, 先の文は (apple, eat) = (2, 1)と表現される.\n",
        "\n",
        "単語をベクトルに変換する方法は他にも色々あり, 知りたい場合はscikit公式ドキュメントを参照する事."
      ],
      "metadata": {
        "id": "mB4zWXaNTygB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words = 'english')\n",
        "vectorizer.fit(X_train)\n",
        "X_train_bow = vectorizer.transform(X_train)\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "print('(テキスト番号, 単語番号) 出現回数')\n",
        "print(X_train_bow[0])\n",
        "print('\\nBoW表現ベクトル')\n",
        "print(X_train_bow[0].toarray())"
      ],
      "metadata": {
        "id": "828vG0G4T5hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# NaiveBayesの実装\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VjTEELvsD5dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikitのnaive_bayesにはBernoulliNB(離散データの2クラス分類)やMultinomialNB(離散データの多クラス分類), GaussianNB(連続データのクラス分類)があるが, 今回は離散データを20種類に分けるのでMultinomialNBを使う."
      ],
      "metadata": {
        "id": "0CoSbbQWD_ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB(alpha = 0.4)  #alphaのデフォルトは1\n",
        "mnb.fit(X_train_bow, y_train)\n",
        "\n",
        "print(f'Train Accuracy: {mnb.score(X_train_bow, y_train):.3f}')\n",
        "print(f'Test Accuracy: {mnb.score(X_test_bow, y_test):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_7suM5YFFOp",
        "outputId": "1b571db5-805c-402b-bee7-f19bd1291a7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.951\n",
            "Test Accuracy: 0.811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnb_small = MultinomialNB(alpha = 0.001)\n",
        "mnb_small.fit(X_train_bow, y_train)\n",
        "\n",
        "print(f'Train Accuracy(small): {mnb_small.score(X_train_bow, y_train):.3f}')\n",
        "print(f'Test Accuracy(small): {mnb_small.score(X_test_bow, y_test):.3f}')\n",
        "\n",
        "mnb_large = MultinomialNB(alpha = 100)\n",
        "mnb_large.fit(X_train_bow, y_train)\n",
        "\n",
        "print(f'Train Accuracy(large): {mnb_large.score(X_train_bow, y_train):.3f}')\n",
        "print(f'Test Accuracy(large): {mnb_large.score(X_test_bow, y_test):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0nCsE4Fzj0",
        "outputId": "e3e33799-0a1a-4671-c90d-1b2c2f260e0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy(small): 0.988\n",
            "Test Accuracy(small): 0.799\n",
            "Train Accuracy(large): 0.747\n",
            "Test Accuracy(large): 0.632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 解説\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "egNUbhcmGiT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "みんな大好きベイズの定理を使う.\n",
        "\n",
        "$$ p(\\theta|D)=\\frac{p(D|\\theta)p(\\theta)}{p(D)} $$\n",
        "\n",
        "今回は, $D$がテキスト, $\\theta$はカテゴリである. 目的は左辺を求めることであるが, それは難しいので右辺から求める. \n",
        "改めてcategoryとtextを使って先の定理を分かりやすくしておこう.\n",
        "\n",
        "$$ p(Category|text) = \\frac{p(Text|Category)p(Category)}{p(Text)} \\quad -①$$\n",
        "\n",
        "それぞれの確率の求め方を説明していく. \n",
        "$$ p(category = c) = \\frac{カテゴリcのテキスト数}{全テキスト数} $$\n",
        "\n",
        "$$ p(Text|Category) = p(word_1, word_2, ..., word_K|Category) $$\n",
        "\n",
        "ここでそれぞれの$word_i$は独立に発生すると仮定する. 本当はそんなことはなく, 'サッカー'と'ゴール'は共起しやすいが, 計算が複雑になるのでしょうがない. 右辺の$word_1, word_2, ..., word_K$は同時確率であるが, ここで互いに独立なので積に展開できる. つまり,\n",
        "\n",
        "$$ p(Text|Category) = \\prod_i p(word_i|Category)$$\n",
        "\n",
        "カテゴリを固定すれば,\n",
        "\n",
        "$$ p(word_i|Category=c) = \\frac{カテゴリcに属するテキスト中に存在するword_iの出現回数}{カテゴリcに属する全単語の出現回数}$$\n",
        "\n",
        "例を見ていこう. \n",
        "\n",
        "\n",
        "1.   テキスト1 | BoW表現 = [0,0,0,...,1]  |カテゴリ7\n",
        "2.   テキスト2 | BoW表現 = [0,0,0,...,0]  |カテゴリ13\n",
        "1.   テキスト3 | BoW表現 = [0,1,1,...,0]  |カテゴリ2\n",
        "2.   テキスト4 | BoW表現 = [1,0,1,...,0]  |カテゴリ3\n",
        "1.   テキスト5 | BoW表現 = [0,0,0,...,0]  |カテゴリ6\n",
        "2.   テキスト6 | BoW表現 = [0,0,1,...,1]  |カテゴリ3\n",
        "                :\n",
        "                :\n",
        "99.   テキスト99 | BoW表現 = [2,0,0,...,0]  |カテゴリ10\n",
        "100.   テキスト100 | BoW表現 = [0,0,0,...,2] | カテゴリ3\n",
        "\n",
        "ここでカテゴリ3に属するテキストが4,6,100番だけだとする. このときそれぞれの確率は\n",
        "\n",
        "$$ p(category=3) = \\frac{カテゴリ3のテキスト数}{全テキスト数} = \\frac{3}{100}$$\n",
        "$$ p(word_3|Category=3) = \\frac{カテゴリ3に属するテキスト中に存在するword_3の出現回数}{カテゴリ3に属する全単語の出現回数} = \\frac{2}{6}$$\n",
        "\n",
        "これを$word_1,word_2,...,word_K$について計算し, 積をとれば$p(Text|Category)$が求まる. このようにして式①を求める. 今はカテゴリ3について計算したが, それをカテゴリ1, カテゴリ2についても計算しそのうち最大のものが分類結果となる. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ここで一つ問題が生じる. 訓練データにない単語がテストデータに出てきたときの挙動を考えてみると, $ p(word_n|Category=c) =0 $になってしまう. そうすると, 積で表される$p(Text|Category=c)$も0になる. つまり新しい単語が存在するだけで、すべてのカテゴリに属する確率が0になってしまい分類ができない(ゼロ頻度問題). これを回避するため, スムージングという手法を用いる. $ p(word_n|Category=c) =0 $とならないように, 計算方法を新しく定義する.\n",
        "\n",
        "$$ p(word_i|Category=c) = \\frac{カテゴリcに属するテキスト中に存在するword_iの出現回数 + \\alpha}{カテゴリcに属する全単語の出現回数 + \\alpha V}$$\n",
        "ここで$V$は全単語の種類数である. 先の例を使うとBoW表現の次元数である. \n",
        "\n",
        "このようにすれば新しい単語が出てきても, つまりカテゴリcに属するテキスト中に存在する$word_i$の出現回数が0でも, $\\alpha$が加算されているので$p(word|category)$は0にはならない. 特に$\\alpha=1$の場合をラプラススムージングという. 実装時に引数として設定したalphaがこの$\\alpha$に相当する. alphaが小さいと新しい単語が出てきたときの$p(word|category)$が0に近づき訓練データにしか対応できなくなってしまう. 逆にalphaが大きすぎると, カテゴリcに属するテキスト中に存在するword_iの出現回数の影響力が弱くなり, 識別能が低くなる.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FDCmFXNhGpa-"
      }
    }
  ]
}