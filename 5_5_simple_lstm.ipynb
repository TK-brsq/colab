{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TKph/colab/blob/main/5_5_simple_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyN2BoVJFFQK"
      },
      "source": [
        "### 5.5.2 全体のコード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMfHfIpmV3F3"
      },
      "source": [
        "import numpy as np\n",
        "# import cupy as np  # GPUの場合\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -- 各設定値 --\n",
        "n_time = 10  # 時系列の数\n",
        "n_in = 1  # 入力層のニューロン数\n",
        "n_mid = 20  # 中間層のニューロン数\n",
        "n_out = 1  # 出力層のニューロン数\n",
        "\n",
        "eta = 0.01  # 学習係数\n",
        "epochs = 101\n",
        "batch_size = 8\n",
        "interval = 10  # 経過の表示間隔\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# -- 訓練データの作成 --\n",
        "sin_x = np.linspace(-2*np.pi, 2*np.pi)  # -2πから2πまで\n",
        "sin_y = np.sin(sin_x)  + 0.1*np.random.randn(len(sin_x))  # sin関数に乱数でノイズを加える\n",
        "n_sample = len(sin_x)-n_time  # サンプル数\n",
        "input_data = np.zeros((n_sample, n_time, n_in))  # 入力\n",
        "correct_data = np.zeros((n_sample, n_out))  # 正解\n",
        "for i in range(0, n_sample):\n",
        "    input_data[i] = sin_y[i:i+n_time].reshape(-1, 1)\n",
        "    correct_data[i] = sin_y[i+n_time:i+n_time+1]  # 正解は入力よりも一つ後\n",
        "\n",
        "# -- LSTM層 --\n",
        "class LSTMLayer:\n",
        "    def __init__(self, n_upper, n):\n",
        "        # 各パラメータの初期値\n",
        "        self.w = np.random.randn(4, n_upper, n) / np.sqrt(n_upper)  # Xavierの初期値\n",
        "        self.v = np.random.randn(4, n, n) / np.sqrt(n)\n",
        "        self.b = np.zeros((4, n))\n",
        "\n",
        "    def forward(self, x, y_prev, c_prev):  # y_prev, c_prev: 前の時刻の出力と記憶セル\n",
        "        u = np.matmul(x, self.w) + np.matmul(y_prev, self.v) + self.b.reshape(4, 1, -1)\n",
        "\n",
        "        a0 = sigmoid(u[0])  # 忘却ゲート\n",
        "        a1 = sigmoid(u[1])  # 入力ゲート\n",
        "        a2 = np.tanh(u[2])  # 新しい記憶\n",
        "        a3 = sigmoid(u[3])  # 出力ゲート\n",
        "        self.gates = np.stack((a0, a1, a2, a3))\n",
        "\n",
        "        self.c = a0*c_prev + a1*a2  # 記憶セル\n",
        "        self.y = a3 * np.tanh(self.c)  # 出力\n",
        "\n",
        "    def backward(self, x, y, c, y_prev, c_prev, gates, grad_y, grad_c):\n",
        "        a0, a1, a2, a3 = gates\n",
        "        tanh_c = np.tanh(c)\n",
        "        r = grad_c + (grad_y*a3) * (1-tanh_c**2)\n",
        "\n",
        "        # 各delta\n",
        "        delta_a0 = r * c_prev * a0 * (1-a0)\n",
        "        delta_a1 = r * a2 * a1 * (1-a1)\n",
        "        delta_a2 = r * a1 * (1 - a2**2)\n",
        "        delta_a3 = grad_y * tanh_c * a3 * (1 - a3)\n",
        "\n",
        "        deltas = np.stack((delta_a0, delta_a1, delta_a2, delta_a3))\n",
        "\n",
        "        # 各パラメータの勾配\n",
        "        self.grad_w += np.matmul(x.T, deltas)\n",
        "        self.grad_v += np.matmul(y_prev.T, deltas)\n",
        "        self.grad_b += np.sum(deltas, axis=1)\n",
        "\n",
        "        # xの勾配\n",
        "        grad_x = np.matmul(deltas, self.w.transpose(0, 2, 1))\n",
        "        self.grad_x = np.sum(grad_x, axis=0)\n",
        "\n",
        "        # y_prevの勾配\n",
        "        grad_y_prev = np.matmul(deltas, self.v.transpose(0, 2, 1))\n",
        "        self.grad_y_prev = np.sum(grad_y_prev, axis=0)\n",
        "\n",
        "        # c_prevの勾配\n",
        "        self.grad_c_prev = r * a0\n",
        "\n",
        "    def reset_sum_grad(self):\n",
        "        self.grad_w = np.zeros_like(self.w)\n",
        "        self.grad_v = np.zeros_like(self.v)\n",
        "        self.grad_b = np.zeros_like(self.b)\n",
        "\n",
        "    def update(self, eta):\n",
        "        self.w -= eta * self.grad_w\n",
        "        self.v -= eta * self.grad_v\n",
        "        self.b -= eta * self.grad_b\n",
        "\n",
        "# -- 全結合 出力層 --\n",
        "class OutputLayer:\n",
        "    def __init__(self, n_upper, n):\n",
        "        self.w = np.random.randn(n_upper, n) / np.sqrt(n_upper)  # Xavierの初期値\n",
        "        self.b = np.zeros(n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        u = np.dot(x, self.w) + self.b\n",
        "        self.y = u  # 恒等関数\n",
        "\n",
        "    def backward(self, t):\n",
        "        delta = self.y - t\n",
        "\n",
        "        self.grad_w = np.dot(self.x.T, delta)\n",
        "        self.grad_b = np.sum(delta, axis=0)\n",
        "        self.grad_x = np.dot(delta, self.w.T)\n",
        "\n",
        "    def update(self, eta):\n",
        "        self.w -= eta * self.grad_w\n",
        "        self.b -= eta * self.grad_b\n",
        "\n",
        "# -- 各層の初期化 --\n",
        "lstm_layer = LSTMLayer(n_in, n_mid)\n",
        "output_layer = OutputLayer(n_mid, n_out)\n",
        "\n",
        "# -- 訓練 --\n",
        "def train(x_mb, t_mb):\n",
        "    # 順伝播 LSTM層\n",
        "    y_rnn = np.zeros((len(x_mb), n_time+1, n_mid))\n",
        "    c_rnn = np.zeros((len(x_mb), n_time+1, n_mid))\n",
        "    gates_rnn = np.zeros((4, len(x_mb), n_time, n_mid))\n",
        "    y_prev = y_rnn[:, 0, :]\n",
        "    c_prev = c_rnn[:, 0, :]\n",
        "    for i in range(n_time):\n",
        "        x = x_mb[:, i, :]\n",
        "        lstm_layer.forward(x, y_prev, c_prev)\n",
        "\n",
        "        y = lstm_layer.y\n",
        "        y_rnn[:, i+1, :] = y\n",
        "        y_prev = y\n",
        "\n",
        "        c = lstm_layer.c\n",
        "        c_rnn[:, i+1, :] = c\n",
        "        c_prev = c\n",
        "\n",
        "        gates = lstm_layer.gates\n",
        "        gates_rnn[:, :, i, :] = gates\n",
        "\n",
        "    # 順伝播 出力層\n",
        "    output_layer.forward(y)\n",
        "\n",
        "    # 逆伝播 出力層\n",
        "    output_layer.backward(t_mb)\n",
        "    grad_y = output_layer.grad_x\n",
        "    grad_c = np.zeros_like(lstm_layer.c)\n",
        "\n",
        "    # 逆伝播 LSTM層\n",
        "    lstm_layer.reset_sum_grad()\n",
        "    for i in reversed(range(n_time)):\n",
        "        x = x_mb[:, i, :]\n",
        "        y = y_rnn[:, i+1, :]\n",
        "        c = c_rnn[:, i+1, :]\n",
        "        y_prev = y_rnn[:, i, :]\n",
        "        c_prev = c_rnn[:, i, :]\n",
        "        gates = gates_rnn[:, :, i, :]\n",
        "\n",
        "        lstm_layer.backward(x, y, c, y_prev, c_prev, gates, grad_y, grad_c)\n",
        "        grad_y = lstm_layer.grad_y_prev\n",
        "        grad_c = lstm_layer.grad_c_prev\n",
        "\n",
        "    # パラメータの更新\n",
        "    lstm_layer.update(eta)\n",
        "    output_layer.update(eta)\n",
        "\n",
        "# -- 予測 --\n",
        "def predict(x_mb):\n",
        "    # 順伝播 LSTM層\n",
        "    y_prev = np.zeros((len(x_mb), n_mid))\n",
        "    c_prev = np.zeros((len(x_mb), n_mid))\n",
        "    for i in range(n_time):\n",
        "        x = x_mb[:, i, :]\n",
        "        lstm_layer.forward(x, y_prev, c_prev)\n",
        "        y = lstm_layer.y\n",
        "        y_prev = y\n",
        "        c = lstm_layer.c\n",
        "        c_prev = c\n",
        "\n",
        "    # 順伝播 出力層\n",
        "    output_layer.forward(y)\n",
        "    return output_layer.y\n",
        "\n",
        "# -- 誤差を計算 --\n",
        "def get_error(x, t):\n",
        "    y = predict(x)\n",
        "    return 1.0/2.0*np.sum(np.square(y - t))  # 二乗和誤差\n",
        "\n",
        "error_record = []\n",
        "n_batch = len(input_data) // batch_size  # 1エポックあたりのバッチ数\n",
        "for i in range(epochs):\n",
        "\n",
        "    # -- 学習 --\n",
        "    index_random = np.arange(len(input_data))\n",
        "    np.random.shuffle(index_random)  # インデックスをシャッフルする\n",
        "    for j in range(n_batch):\n",
        "\n",
        "        # ミニバッチを取り出す\n",
        "        mb_index = index_random[j*batch_size : (j+1)*batch_size]\n",
        "        x_mb = input_data[mb_index, :]\n",
        "        t_mb = correct_data[mb_index, :]\n",
        "        train(x_mb, t_mb)\n",
        "\n",
        "    # -- 誤差を求める --\n",
        "    error = get_error(input_data, correct_data)\n",
        "    error_record.append(error)\n",
        "\n",
        "    # -- 経過の表示 --\n",
        "    if i%interval == 0:\n",
        "        print(\"Epoch:\"+str(i+1)+\"/\"+str(epochs), \"Error:\"+str(error))\n",
        "\n",
        "        predicted = input_data[0].reshape(-1).tolist() # 最初の入力\n",
        "        for i in range(n_sample):\n",
        "            x = np.array(predicted[-n_time:]).reshape(1, n_time, 1)\n",
        "            y = predict(x)\n",
        "            predicted.append(float(y[0, 0]))  # 出力をpredictedに追加する\n",
        "\n",
        "        plt.plot(range(len(sin_y)), sin_y.tolist(), label=\"Correct\")\n",
        "        plt.plot(range(len(predicted)), predicted, label=\"Predicted\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "plt.plot(range(1, len(error_record)+1), error_record)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}